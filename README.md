
# CIFAR-10 ViT åˆ†å¸ƒå¼è®­ç»ƒ

æœ¬é¡¹ç›®ä½¿ç”¨ **PyTorch** å®ç° **Vision Transformer (ViT)** åœ¨ **CIFAR-10** æ•°æ®é›†ä¸Šçš„è®­ç»ƒï¼Œæ”¯æŒ **å¤š GPU åˆ†å¸ƒå¼è®­ç»ƒ (DDP)**ï¼Œå¹¶é›†æˆæ•°æ®åŠ è½½ã€æ—¥å¿—è®°å½•å’Œæ¨¡å‹ä¿å­˜åŠŸèƒ½ã€‚

---

## ğŸ“¦ åŠŸèƒ½
- åŸºäº CIFAR-10 çš„ ViT æ¨¡å‹ã€‚
- PyTorch **DDP** åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ”¯æŒå¤š GPU åŠ é€Ÿã€‚
- **CosineAnnealingLR** å­¦ä¹ ç‡è°ƒåº¦ã€‚
- æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰ä¿è¯è®­ç»ƒç¨³å®šã€‚
- ä½¿ç”¨ **SwanLab** è¿›è¡Œå®éªŒæ—¥å¿—è®°å½•ï¼ˆå¯é€‰ï¼‰ã€‚
- æ¨¡å‹æ£€æŸ¥ç‚¹ä¿å­˜ä¸æ¢å¤ã€‚

---

## ğŸ–¼ æ•°æ®é›†
ä½¿ç”¨ **CIFAR-10**ï¼š
- è®­ç»ƒé›†ï¼š50,000 å¼ å›¾ç‰‡
- æµ‹è¯•é›†ï¼š10,000 å¼ å›¾ç‰‡
- å°†å›¾ç‰‡ç»Ÿä¸€ç¼©æ”¾åˆ° **224Ã—224**ï¼Œé€‚é… ViT è¾“å…¥ã€‚
- å›¾åƒå½’ä¸€åŒ–å‚æ•°ï¼š
```python
IMAGENET_MEAN = [0.485, 0.456, 0.406]
IMAGENET_STD = [0.229, 0.224, 0.225]
```

---

## âš™ï¸ ç¯å¢ƒ
- Python >= 3.9
- PyTorch >= 2.0
- torchvision
- CUDA GPUï¼ˆæ”¯æŒ DDPï¼‰
- SwanLabï¼ˆå¯é€‰ï¼Œç”¨äºæ—¥å¿—è®°å½•ï¼‰

---

## ğŸš€ è®­ç»ƒ
<img width="2085" height="767" alt="image" src="https://github.com/user-attachments/assets/e3a7a55c-41b0-479f-bf37-1f5b6c1761d5" />

<img width="2084" height="850" alt="image" src="https://github.com/user-attachments/assets/fbc6cb49-104f-4e55-aa99-ec4623886bba" />

<img width="2081" height="844" alt="image" src="https://github.com/user-attachments/assets/6bf8f737-6b3a-4ce1-be2a-6425ee36edc6" />

## æ¨ç†
```
Rank 0 loaded 10000 samples.
Rank 0: æ­£åœ¨åŠ è½½æ¨¡å‹æƒé‡...
Rank 2 loaded 10000 samples.
Rank 3 loaded 10000 samples.
Rank 1 loaded 10000 samples.
```

```
================ æœ€ç»ˆæ¨ç†ç»“æœ ================
æ€»æ ·æœ¬æ•° (Total Samples): 10000
å…¨å±€å¹³å‡æŸå¤± (Global Average Loss): 0.7909
å…¨å±€å‡†ç¡®ç‡ (Global Accuracy): 0.7300
================================================
```

### 1. å¤š GPU å¯åŠ¨
ä½¿ç”¨ `torchrun`ï¼š
```bash
torchrun --nproc_per_node=NUM_GPUS train.py
```
å°† `NUM_GPUS` æ›¿æ¢ä¸ºå®é™…ä½¿ç”¨çš„ GPU æ•°é‡ã€‚

### 2. è¶…å‚æ•°é…ç½®
åœ¨ `config.py` ä¸­ï¼š
- `EPOCHES = 50`
- `BATCH_SIZE = 256`
- `LEARNING_RATE = 1e-3`
- `EMB_SIZE = 768`
- `NUM_HEADS = 12`
- `NUM_LAYERS = 12`
- Patch å¤§å°ï¼š16

### 3. å­¦ä¹ ç‡è°ƒåº¦å™¨
ä½¿ç”¨ **CosineAnnealingLR**ï¼š
```python
scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHES)
```
åœ¨æ¯ä¸ª epoch ç»“æŸåè°ƒç”¨ï¼š
```python
scheduler.step()
```

---

## ğŸ§  æ¨¡å‹ç»“æ„ï¼ˆæä¾›å®˜æ–¹å’Œè‡ªå·±å®ç°çš„ï¼‰
- **Patch Embedding**ï¼šé€šè¿‡ `Conv2d` å°†å›¾ç‰‡åˆ’åˆ† patch å¹¶æ˜ å°„åˆ° embeddingã€‚
- **CLS Token** + ä½ç½®ç¼–ç ã€‚
- **Transformer ç¼–ç å™¨** (`nn.TransformerEncoder`)ã€‚
- **åˆ†ç±»å¤´**ï¼šä»…ä½¿ç”¨ [CLS] token è¾“å‡ºã€‚

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜
- æ¯ä¸ª epoch ä¿å­˜ä¸€æ¬¡ï¼ˆä»… rank 0ï¼‰ï¼š
```python
model_save(model.module if hasattr(model, "module") else model, '.model.pth', rank)
```
- æœ€æ–°æ¨¡å‹é‡å‘½åä¸º `model.pth`ã€‚

---

## ğŸ“Š éªŒè¯
- `validate()` åœ¨ **æ— æ¢¯åº¦**ä¸‹è¿è¡Œã€‚
- æ”¯æŒ DDP å…¨å±€èšåˆã€‚
- è¾“å‡º **å¹³å‡æŸå¤±** å’Œ **å‡†ç¡®ç‡**ã€‚

---

## âš¡ ä½¿ç”¨æç¤º
- CIFAR-10 æ•°æ®é‡è¾ƒå°ï¼ŒViT-Base å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œå¯è°ƒæ•´ï¼š
  - å‡å° embedding å°ºå¯¸ (`emb_size=256`)
  - å‡å°‘å±‚æ•° (`num_layers=6`)
  - å‡å°‘æ³¨æ„åŠ›å¤´ (`num_heads=8`)
- æ¢¯åº¦è£å‰ªä¿è¯è®­ç»ƒç¨³å®šï¼š

```python
clip_grad_norm_(model.parameters(), max_norm=1.0)
```

---

## ğŸ“œ å‚è€ƒèµ„æ–™
- [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)
- [PyTorch Distributed Data Parallel](https://pytorch.org/docs/stable/notes/ddp.html)
- [CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)
- https://github.com/owenliang/mnist-vit
- https://www.bilibili.com/video/BV1fH4y1H7mV/?spm_id_from=333.1391.0.0




æœ¬é¡¹ç›®ä»…ç”¨äºç§‘ç ”ä¸å­¦ä¹ ã€‚


